{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 практическое задание. Обучение полносвязной нейронной сети.\n",
    "\n",
    "## Практикум на ЭВМ для 317 группы, весна 2019\n",
    "\n",
    "#### Фамилия, имя: Петренко Дарья\n",
    "\n",
    "Дата выдачи: 19 февраля\n",
    "\n",
    "Мягкий дедлайн: 28 февраля 23:59 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация нейронной сети (6 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вы обучите полносвязную нейронную сеть распознавать рукописные цифры (а что же еще, если не их :), [почти] самостоятельно реализовав все составляющие алгоритма обучения и предсказания.\n",
    "\n",
    "Для начала нам понадобится реализовать прямой и обратный проход через слои. Наши слои будут соответствовать следующему интерфейсу (на примере \"тождественного\" слоя):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityLayer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           \n",
    "    output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    \n",
    "    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters.\n",
    "    \n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) \n",
    "        and auxiliary stuff. You should enumerate all parameters\n",
    "        in self.params\"\"\"\n",
    "        # An identity layer does nothing\n",
    "        self.params = []\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], \n",
    "        returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # An identity layer just returns whatever it gets as input.\n",
    "        self.input = input\n",
    "        return input\n",
    "\n",
    "    def backward(self, grad_output): \n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, \n",
    "        with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, \n",
    "        you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d input  = (d loss / d layer) *  (d layer / d input)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, \n",
    "        so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        The method returns:\n",
    "        * gradient w.r.t input (will be passed to \n",
    "          previous layer's backward method)\n",
    "        * flattened gradient w.r.t. parameters (with .ravel() \n",
    "          applied to each gradient). \n",
    "          If there are no params, return []\n",
    "        \"\"\"\n",
    "        # The gradient of an identity layer is precisely grad_output\n",
    "        input_dim = self.input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(input_dim)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input), [] # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слой нелинейности ReLU\n",
    "\n",
    "Для начала реализуем слой нелинейности $ReLU(x) = max(x, 0)$. Параметров у слоя нет. Метод forward должен вернуть результат поэлементного применения ReLU к входному массиву, метод backward - градиент функции потерь по входу слоя. В нуле будем считать производную равной 0. Обратите внимание, что при обратном проходе могут понадобиться величины, посчитанные во время прямого прохода, поэтому их стоит сохранить как атрибут класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        self.params = [] # ReLU has no parameters\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, num_units] matrix\"\"\"\n",
    "        self.input = input\n",
    "        zero_matrix = np.zeros(np.shape(input))\n",
    "        self.forward_matrix = np.maximum(input, zero_matrix)\n",
    "        return self.forward_matrix \n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\n",
    "        grad_output shape: [batch, num_units]\n",
    "        output 1 shape: [batch, num_units]\n",
    "        output 2: []\n",
    "        \"\"\"\n",
    "        d_layer_d_input = self.forward_matrix != 0\n",
    "        return grad_output * d_layer_d_input, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полносвязный слой\n",
    "Далее реализуем полносвязный слой без нелинейности. У слоя два параметра: матрица весов и вектор сдвига.\n",
    "\n",
    "Обратите внимание на второй аргумент: в нем надо возвращать градиент по всем параметрам в одномерном виде. Для этого надо сначала применить .ravel() ко всем градиентам, а затем воспользоваться  np.r_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 2., 3.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "np.r_[np.eye(3).ravel(), np.arange(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    \"\"\"\n",
    "    Modified code from cs.hse DL course *\n",
    "    \"\"\"\n",
    "    def __init__(self, input_units, output_units):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \"\"\"\n",
    "        # initialize weights with small random numbers from normal distribution\n",
    "        #self.weights = np.random.randn(input_units, output_units)*0.01\n",
    "        self.weights = np.random.normal(loc=0.0, scale=(2/(input_units+output_units)), size=(input_units, output_units) )\n",
    "        self.biases = np.zeros(output_units)\n",
    "        self.params = [self.weights, self.biases]\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = W x + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self.forward_matrix = np.dot(input, self.weights) + self.biases[np.newaxis, :]\n",
    "        return self.forward_matrix\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        compute gradients\n",
    "        grad_output shape: [batch, output_units]\n",
    "        output shapes: [batch, input_units], [num_params]\n",
    "        \n",
    "        hint: use function np.r_\n",
    "        np.r_[np.arange(3), np.arange(3)] = [0, 1, 2, 0, 1, 2]\n",
    "        \"\"\"\n",
    "        d_loss_d_w = np.dot(np.transpose(self.input), grad_output)\n",
    "        d_loss_d_b = np.dot(np.ones(np.size(grad_output, 0))[np.newaxis, :], grad_output)\n",
    "        d_loss_d_input = np.dot(grad_output, np.transpose(self.weights))\n",
    "        return d_loss_d_input, np.r_[d_loss_d_w.ravel(), d_loss_d_b.ravel()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка градиента\n",
    "\n",
    "Проверим правильность реализации с помощью функции численной проверки градиента. Функция берет на вход callable объект (функцию от одного аргумента-матрицы) и аргумент и вычисляет приближенный градиент функции в этой точке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_numerical_gradient(f, x, verbose=False, h=0.00001):\n",
    "    \"\"\"Evaluates gradient df/dx via finite differences:\n",
    "    df/dx ~ (f(x+h) - f(x-h)) / 2h\n",
    "    Adopted from https://github.com/ddtm/dl-course/\n",
    "    \"\"\"\n",
    "    fx = f(x) # evaluate function value at original point\n",
    "    grad = np.zeros_like(x)\n",
    "    # iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "\n",
    "        # evaluate function at x+h\n",
    "        ix = it.multi_index\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h # increment by h\n",
    "        fxph = f(x) # evalute f(x + h)\n",
    "        x[ix] = oldval - h\n",
    "        fxmh = f(x) # evaluate f(x - h)\n",
    "        x[ix] = oldval # restore\n",
    "\n",
    "        # compute the partial derivative with centered formula\n",
    "        grad[ix] = (fxph - fxmh) / (2 * h) # the slope\n",
    "        if verbose:\n",
    "            print (ix, grad[ix])\n",
    "        it.iternext() # step to next dimension\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу слоя ReLU от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = ReLU(x) $$\n",
    "\n",
    "Следующая ячейка после заполнения должна не выдавать ошибку :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "\n",
    "def func(x):\n",
    "    zero_matrix = np.zeros(np.shape(x))\n",
    "    y = np.maximum(x, zero_matrix)\n",
    "    return np.sum(y)\n",
    "\n",
    "d_loss_d_layer = np.ones(np.shape(points))\n",
    "relu = ReLU()\n",
    "relu.forward(points)\n",
    "grads = relu.backward(d_loss_d_layer)[0]\n",
    "numeric_grads = eval_numerical_gradient(func, points)\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите аналитический и численный градиенты по входу полносвязного слоя от функции\n",
    "$$ f(y) = \\sum_i y_i, \\quad y = W x + b $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "dense = Dense(12, 32,)\n",
    "\n",
    "def func(x):\n",
    "    w = dense.weights\n",
    "    b = dense.biases\n",
    "    y = np.dot(x, w) + b[np.newaxis, :]\n",
    "    return np.sum(y)\n",
    "    \n",
    "d_loss_d_layer = np.ones((np.size(x, 0), 32))\n",
    "dense.forward(x)\n",
    "grads = dense.backward(d_loss_d_layer)[0]\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(func, x)\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Реализация softmax-слоя и функции потерь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи многоклассовой классификации обычно используют softmax в качестве нелинейности на последнем слое, чтобы получить вероятности классов для каждого объекта:\n",
    "$$\\hat y = softmax(x)  = \\bigl \\{\\frac {exp(x_i)}{\\sum_j exp(x_j)} \\bigr \\}_{i=1}^K, \\quad K - \\text{число классов}$$\n",
    "В этом случае удобно оптимизировать логарифм правдоподобия:\n",
    "$$L(y, \\hat y) = -\\sum_{i=1}^K y_i \\log \\hat y_i \\rightarrow \\min,$$\n",
    "где $y_i=1$, если объект принадлежит $i$-му классу, и 0 иначе. Записанная в таком виде, эта функция потерь совпадает с выражением для кросс-энтропии. Очевидно, что ее также можно переписать через индексацию, если через $y_i$ обозначить класс данного объекта:\n",
    "$$L(y, \\hat y) = - \\log \\hat y_{y_i} \\rightarrow \\min$$\n",
    "В таком виде ее удобно реализовывать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте слой Softmax (без параметров). Метод forward должен вычислять логарифм от softmax, а метод backward - пропускать градиенты. В общем случае в промежуточных вычислениях backward получится трехмерный тензор, однако для нашей конкретной функции потерь все вычисления можно реализовать в матричном виде.  Поэтому мы будем предполагать, что аргумент grad_output - это матрица, у которой в каждой строке только одно ненулевое значение (не обязательно единица)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "# use this function instead of np.log(np.sum(np.exp(...))) !\n",
    "# because it is more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Applies softmax to each row and then applies component-wise log\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self.forward_matrix = input - logsumexp(input, axis=1)[:, np.newaxis]\n",
    "        return self.forward_matrix\n",
    "        \n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        \"\"\"\n",
    "        Propagartes gradients.\n",
    "        Assumes that each row of grad_output contains only 1 \n",
    "        non-zero element\n",
    "        Input shape: [batch, num_units]\n",
    "        Output shape: [batch, num_units]\n",
    "        Do not forget to return [] as second value (grad w.r.t. params)\n",
    "        \"\"\"\n",
    "        exp_matr = np.exp(self.input)\n",
    "        d_layer_d_input = grad_output + exp_matr / np.sum(exp_matr, axis=1)[:, np.newaxis]\n",
    "        return d_layer_d_input, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию потерь и градиенты функции потерь. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns negative log-likelihood of target under model represented by\n",
    "    activations (log probabilities of classes)\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: 1 (scalar)\n",
    "    \"\"\"\n",
    "    return -1 * np.sum(activations * target)\n",
    "    \n",
    "\n",
    "def grad_crossentropy(activations, target):\n",
    "    \"\"\"\n",
    "    returns gradient of negative log-likelihood w.r.t. activations\n",
    "    each arg has shape [batch, num_classes]\n",
    "    output shape: [batch, num_classes]\n",
    "    \n",
    "    hint: this is just one-hot encoding of target vector\n",
    "          multiplied by -1\n",
    "    \"\"\"\n",
    "    return -1 * target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, выполните проверку softmax-слоя, используя функцию потерь и ее градиент.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "points = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "target = np.arange(10)\n",
    "enc = LabelBinarizer().fit(np.arange(12))\n",
    "target = enc.transform(target)\n",
    "\n",
    "def func(x):\n",
    "    z = x - logsumexp(x, axis=1)[:, np.newaxis]\n",
    "    return crossentropy(z, target)\n",
    "\n",
    "\n",
    "softmax = Softmax()\n",
    "z = softmax.forward(points)\n",
    "d_loss_d_layer = grad_crossentropy(z, target)\n",
    "grads = softmax.backward(d_loss_d_layer)[0]\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(func, points)\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных\n",
    "\n",
    "Мы реализаовали все архитектурные составляющие нашей нейронной сети. Осталось загрузить данные и обучить модель. Мы будем работать с датасетом digits, каждый объект в котором - это 8x8 изображение рукописной цифры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_digits(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1797, 64), (1797,))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучение и контроль:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1347, 64), (450, 64))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one-hot encoding векторов значений целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = LabelBinarizer().fit(np.arange(10))\n",
    "y_train_ohe = enc.transform(y_train)\n",
    "y_test_ohe = enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сборка и обучение нейронной сети (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нашей реализации нейросеть - это список слоев. Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "hidden_layers_size = 32\n",
    "network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(hidden_layers_size, 10))\n",
    "network.append(Softmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для проверки, хорошо ли сеть обучилась, нам понадобится вычислять точность (accuracy) на данной выборке. Для этого реализуйте функцию, которая делает предсказания на каждом объекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    returns predictions for each object in X\n",
    "    network: list of layers\n",
    "    X: raw data\n",
    "    X shape: [batch, features_num]\n",
    "    output: array of classes, each from 0 to 9\n",
    "    output shape: [batch]\n",
    "    \"\"\"\n",
    "    curr = X\n",
    "    for layer in network:\n",
    "        curr = layer.forward(curr)\n",
    "    return np.argmax(curr, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы будем обучать параметры нейросети с помощью готовой функции оптимизации из модуля scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function minimize in module scipy.optimize._minimize:\n",
      "\n",
      "minimize(fun, x0, args=(), method=None, jac=None, hess=None, hessp=None, bounds=None, constraints=(), tol=None, callback=None, options=None)\n",
      "    Minimization of scalar function of one or more variables.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    fun : callable\n",
      "        The objective function to be minimized.\n",
      "    \n",
      "            ``fun(x, *args) -> float``\n",
      "    \n",
      "        where x is an 1-D array with shape (n,) and `args`\n",
      "        is a tuple of the fixed parameters needed to completely\n",
      "        specify the function.\n",
      "    x0 : ndarray, shape (n,)\n",
      "        Initial guess. Array of real elements of size (n,),\n",
      "        where 'n' is the number of independent variables.\n",
      "    args : tuple, optional\n",
      "        Extra arguments passed to the objective function and its\n",
      "        derivatives (`fun`, `jac` and `hess` functions).\n",
      "    method : str or callable, optional\n",
      "        Type of solver.  Should be one of\n",
      "    \n",
      "            - 'Nelder-Mead' :ref:`(see here) <optimize.minimize-neldermead>`\n",
      "            - 'Powell'      :ref:`(see here) <optimize.minimize-powell>`\n",
      "            - 'CG'          :ref:`(see here) <optimize.minimize-cg>`\n",
      "            - 'BFGS'        :ref:`(see here) <optimize.minimize-bfgs>`\n",
      "            - 'Newton-CG'   :ref:`(see here) <optimize.minimize-newtoncg>`\n",
      "            - 'L-BFGS-B'    :ref:`(see here) <optimize.minimize-lbfgsb>`\n",
      "            - 'TNC'         :ref:`(see here) <optimize.minimize-tnc>`\n",
      "            - 'COBYLA'      :ref:`(see here) <optimize.minimize-cobyla>`\n",
      "            - 'SLSQP'       :ref:`(see here) <optimize.minimize-slsqp>`\n",
      "            - 'trust-constr':ref:`(see here) <optimize.minimize-trustconstr>`\n",
      "            - 'dogleg'      :ref:`(see here) <optimize.minimize-dogleg>`\n",
      "            - 'trust-ncg'   :ref:`(see here) <optimize.minimize-trustncg>`\n",
      "            - 'trust-exact' :ref:`(see here) <optimize.minimize-trustexact>`\n",
      "            - 'trust-krylov' :ref:`(see here) <optimize.minimize-trustkrylov>`\n",
      "            - custom - a callable object (added in version 0.14.0),\n",
      "              see below for description.\n",
      "    \n",
      "        If not given, chosen to be one of ``BFGS``, ``L-BFGS-B``, ``SLSQP``,\n",
      "        depending if the problem has constraints or bounds.\n",
      "    jac : {callable,  '2-point', '3-point', 'cs', bool}, optional\n",
      "        Method for computing the gradient vector. Only for CG, BFGS,\n",
      "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg, trust-krylov,\n",
      "        trust-exact and trust-constr. If it is a callable, it should be a\n",
      "        function that returns the gradient vector:\n",
      "    \n",
      "            ``jac(x, *args) -> array_like, shape (n,)``\n",
      "    \n",
      "        where x is an array with shape (n,) and `args` is a tuple with\n",
      "        the fixed parameters. Alternatively, the keywords\n",
      "        {'2-point', '3-point', 'cs'} select a finite\n",
      "        difference scheme for numerical estimation of the gradient. Options\n",
      "        '3-point' and 'cs' are available only to 'trust-constr'.\n",
      "        If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
      "        gradient along with the objective function. If False, the gradient\n",
      "        will be estimated using '2-point' finite difference estimation.\n",
      "    hess : {callable, '2-point', '3-point', 'cs', HessianUpdateStrategy},  optional\n",
      "        Method for computing the Hessian matrix. Only for Newton-CG, dogleg,\n",
      "        trust-ncg,  trust-krylov, trust-exact and trust-constr. If it is\n",
      "        callable, it should return the  Hessian matrix:\n",
      "    \n",
      "            ``hess(x, *args) -> {LinearOperator, spmatrix, array}, (n, n)``\n",
      "    \n",
      "        where x is a (n,) ndarray and `args` is a tuple with the fixed\n",
      "        parameters. LinearOperator and sparse matrix returns are\n",
      "        allowed only for 'trust-constr' method. Alternatively, the keywords\n",
      "        {'2-point', '3-point', 'cs'} select a finite difference scheme\n",
      "        for numerical estimation. Or, objects implementing\n",
      "        `HessianUpdateStrategy` interface can be used to approximate\n",
      "        the Hessian. Available quasi-Newton methods implementing\n",
      "        this interface are:\n",
      "    \n",
      "            - `BFGS`;\n",
      "            - `SR1`.\n",
      "    \n",
      "        Whenever the gradient is estimated via finite-differences,\n",
      "        the Hessian cannot be estimated with options\n",
      "        {'2-point', '3-point', 'cs'} and needs to be\n",
      "        estimated using one of the quasi-Newton strategies.\n",
      "        Finite-difference options {'2-point', '3-point', 'cs'} and\n",
      "        `HessianUpdateStrategy` are available only for 'trust-constr' method.\n",
      "    hessp : callable, optional\n",
      "        Hessian of objective function times an arbitrary vector p. Only for\n",
      "        Newton-CG, trust-ncg, trust-krylov, trust-constr.\n",
      "        Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
      "        provided, then `hessp` will be ignored.  `hessp` must compute the\n",
      "        Hessian times an arbitrary vector:\n",
      "    \n",
      "            ``hessp(x, p, *args) ->  ndarray shape (n,)``\n",
      "    \n",
      "        where x is a (n,) ndarray, p is an arbitrary vector with\n",
      "        dimension (n,) and `args` is a tuple with the fixed\n",
      "        parameters.\n",
      "    bounds : sequence or `Bounds`, optional\n",
      "        Bounds on variables for L-BFGS-B, TNC, SLSQP and\n",
      "        trust-constr methods. There are two ways to specify the bounds:\n",
      "    \n",
      "            1. Instance of `Bounds` class.\n",
      "            2. Sequence of ``(min, max)`` pairs for each element in `x`. None\n",
      "               is used to specify no bound.\n",
      "    \n",
      "    constraints : {Constraint, dict} or List of {Constraint, dict}, optional\n",
      "        Constraints definition (only for COBYLA, SLSQP and trust-constr).\n",
      "        Constraints for 'trust-constr' are defined as a single object or a\n",
      "        list of objects specifying constraints to the optimization problem.\n",
      "        Available constraints are:\n",
      "    \n",
      "            - `LinearConstraint`\n",
      "            - `NonlinearConstraint`\n",
      "    \n",
      "        Constraints for COBYLA, SLSQP are defined as a list of dictionaries.\n",
      "        Each dictionary with fields:\n",
      "    \n",
      "            type : str\n",
      "                Constraint type: 'eq' for equality, 'ineq' for inequality.\n",
      "            fun : callable\n",
      "                The function defining the constraint.\n",
      "            jac : callable, optional\n",
      "                The Jacobian of `fun` (only for SLSQP).\n",
      "            args : sequence, optional\n",
      "                Extra arguments to be passed to the function and Jacobian.\n",
      "    \n",
      "        Equality constraint means that the constraint function result is to\n",
      "        be zero whereas inequality means that it is to be non-negative.\n",
      "        Note that COBYLA only supports inequality constraints.\n",
      "    tol : float, optional\n",
      "        Tolerance for termination. For detailed control, use solver-specific\n",
      "        options.\n",
      "    options : dict, optional\n",
      "        A dictionary of solver options. All methods accept the following\n",
      "        generic options:\n",
      "    \n",
      "            maxiter : int\n",
      "                Maximum number of iterations to perform.\n",
      "            disp : bool\n",
      "                Set to True to print convergence messages.\n",
      "    \n",
      "        For method-specific options, see :func:`show_options()`.\n",
      "    callback : callable, optional\n",
      "        Called after each iteration. For 'trust-constr' it is a callable with\n",
      "        the signature:\n",
      "    \n",
      "            ``callback(xk, OptimizeResult state) -> bool``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector. and ``state``\n",
      "        is an `OptimizeResult` object, with the same fields\n",
      "        as the ones from the return.  If callback returns True\n",
      "        the algorithm execution is terminated.\n",
      "        For all the other methods, the signature is:\n",
      "    \n",
      "            ``callback(xk)``\n",
      "    \n",
      "        where ``xk`` is the current parameter vector.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    res : OptimizeResult\n",
      "        The optimization result represented as a ``OptimizeResult`` object.\n",
      "        Important attributes are: ``x`` the solution array, ``success`` a\n",
      "        Boolean flag indicating if the optimizer exited successfully and\n",
      "        ``message`` which describes the cause of the termination. See\n",
      "        `OptimizeResult` for a description of other attributes.\n",
      "    \n",
      "    \n",
      "    See also\n",
      "    --------\n",
      "    minimize_scalar : Interface to minimization algorithms for scalar\n",
      "        univariate functions\n",
      "    show_options : Additional options accepted by the solvers\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This section describes the available solvers that can be selected by the\n",
      "    'method' parameter. The default method is *BFGS*.\n",
      "    \n",
      "    **Unconstrained minimization**\n",
      "    \n",
      "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
      "    Simplex algorithm [1]_, [2]_. This algorithm is robust in many\n",
      "    applications. However, if numerical computation of derivative can be\n",
      "    trusted, other algorithms using the first and/or second derivatives\n",
      "    information might be preferred for their better performance in\n",
      "    general.\n",
      "    \n",
      "    Method :ref:`Powell <optimize.minimize-powell>` is a modification\n",
      "    of Powell's method [3]_, [4]_ which is a conjugate direction\n",
      "    method. It performs sequential one-dimensional minimizations along\n",
      "    each vector of the directions set (`direc` field in `options` and\n",
      "    `info`), which is updated at each iteration of the main\n",
      "    minimization loop. The function need not be differentiable, and no\n",
      "    derivatives are taken.\n",
      "    \n",
      "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
      "    gradient algorithm by Polak and Ribiere, a variant of the\n",
      "    Fletcher-Reeves method described in [5]_ pp.  120-122. Only the\n",
      "    first derivatives are used.\n",
      "    \n",
      "    Method :ref:`BFGS <optimize.minimize-bfgs>` uses the quasi-Newton\n",
      "    method of Broyden, Fletcher, Goldfarb, and Shanno (BFGS) [5]_\n",
      "    pp. 136. It uses the first derivatives only. BFGS has proven good\n",
      "    performance even for non-smooth optimizations. This method also\n",
      "    returns an approximation of the Hessian inverse, stored as\n",
      "    `hess_inv` in the OptimizeResult object.\n",
      "    \n",
      "    Method :ref:`Newton-CG <optimize.minimize-newtoncg>` uses a\n",
      "    Newton-CG algorithm [5]_ pp. 168 (also known as the truncated\n",
      "    Newton method). It uses a CG method to the compute the search\n",
      "    direction. See also *TNC* method for a box-constrained\n",
      "    minimization with a similar algorithm. Suitable for large-scale\n",
      "    problems.\n",
      "    \n",
      "    Method :ref:`dogleg <optimize.minimize-dogleg>` uses the dog-leg\n",
      "    trust-region algorithm [5]_ for unconstrained minimization. This\n",
      "    algorithm requires the gradient and Hessian; furthermore the\n",
      "    Hessian is required to be positive definite.\n",
      "    \n",
      "    Method :ref:`trust-ncg <optimize.minimize-trustncg>` uses the\n",
      "    Newton conjugate gradient trust-region algorithm [5]_ for\n",
      "    unconstrained minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-krylov <optimize.minimize-trustkrylov>` uses\n",
      "    the Newton GLTR trust-region algorithm [14]_, [15]_ for unconstrained\n",
      "    minimization. This algorithm requires the gradient\n",
      "    and either the Hessian or a function that computes the product of\n",
      "    the Hessian with a given vector. Suitable for large-scale problems.\n",
      "    On indefinite problems it requires usually less iterations than the\n",
      "    `trust-ncg` method and is recommended for medium and large-scale problems.\n",
      "    \n",
      "    Method :ref:`trust-exact <optimize.minimize-trustexact>`\n",
      "    is a trust-region method for unconstrained minimization in which\n",
      "    quadratic subproblems are solved almost exactly [13]_. This\n",
      "    algorithm requires the gradient and the Hessian (which is\n",
      "    *not* required to be positive definite). It is, in many\n",
      "    situations, the Newton method to converge in fewer iteraction\n",
      "    and the most recommended for small and medium-size problems.\n",
      "    \n",
      "    **Bound-Constrained minimization**\n",
      "    \n",
      "    Method :ref:`L-BFGS-B <optimize.minimize-lbfgsb>` uses the L-BFGS-B\n",
      "    algorithm [6]_, [7]_ for bound constrained minimization.\n",
      "    \n",
      "    Method :ref:`TNC <optimize.minimize-tnc>` uses a truncated Newton\n",
      "    algorithm [5]_, [8]_ to minimize a function with variables subject\n",
      "    to bounds. This algorithm uses gradient information; it is also\n",
      "    called Newton Conjugate-Gradient. It differs from the *Newton-CG*\n",
      "    method described above as it wraps a C implementation and allows\n",
      "    each variable to be given upper and lower bounds.\n",
      "    \n",
      "    **Constrained Minimization**\n",
      "    \n",
      "    Method :ref:`COBYLA <optimize.minimize-cobyla>` uses the\n",
      "    Constrained Optimization BY Linear Approximation (COBYLA) method\n",
      "    [9]_, [10]_, [11]_. The algorithm is based on linear\n",
      "    approximations to the objective function and each constraint. The\n",
      "    method wraps a FORTRAN implementation of the algorithm. The\n",
      "    constraints functions 'fun' may return either a single number\n",
      "    or an array or list of numbers.\n",
      "    \n",
      "    Method :ref:`SLSQP <optimize.minimize-slsqp>` uses Sequential\n",
      "    Least SQuares Programming to minimize a function of several\n",
      "    variables with any combination of bounds, equality and inequality\n",
      "    constraints. The method wraps the SLSQP Optimization subroutine\n",
      "    originally implemented by Dieter Kraft [12]_. Note that the\n",
      "    wrapper handles infinite values in bounds by converting them into\n",
      "    large floating values.\n",
      "    \n",
      "    Method :ref:`trust-constr <optimize.minimize-trustconstr>` is a\n",
      "    trust-region algorithm for constrained optimization. It swiches\n",
      "    between two implementations depending on the problem definition.\n",
      "    It is the most versatile constrained minimization algorithm\n",
      "    implemented in SciPy and the most appropriate for large-scale problems.\n",
      "    For equality constrained problems it is an implementation of Byrd-Omojokun\n",
      "    Trust-Region SQP method described in [17]_ and in [5]_, p. 549. When\n",
      "    inequality constraints  are imposed as well, it swiches to the trust-region\n",
      "    interior point  method described in [16]_. This interior point algorithm,\n",
      "    in turn, solves inequality constraints by introducing slack variables\n",
      "    and solving a sequence of equality-constrained barrier problems\n",
      "    for progressively smaller values of the barrier parameter.\n",
      "    The previously described equality constrained SQP method is\n",
      "    used to solve the subproblems with increasing levels of accuracy\n",
      "    as the iterate gets closer to a solution.\n",
      "    \n",
      "    **Finite-Difference Options**\n",
      "    \n",
      "    For Method :ref:`trust-constr <optimize.minimize-trustconstr>`\n",
      "    the gradient and the Hessian may be approximated using\n",
      "    three finite-difference schemes: {'2-point', '3-point', 'cs'}.\n",
      "    The scheme 'cs' is, potentially, the most accurate but it\n",
      "    requires the function to correctly handles complex inputs and to\n",
      "    be differentiable in the complex plane. The scheme '3-point' is more\n",
      "    accurate than '2-point' but requires twice as much operations.\n",
      "    \n",
      "    **Custom minimizers**\n",
      "    \n",
      "    It may be useful to pass a custom minimization method, for example\n",
      "    when using a frontend to this method such as `scipy.optimize.basinhopping`\n",
      "    or a different library.  You can simply pass a callable as the ``method``\n",
      "    parameter.\n",
      "    \n",
      "    The callable is called as ``method(fun, x0, args, **kwargs, **options)``\n",
      "    where ``kwargs`` corresponds to any other parameters passed to `minimize`\n",
      "    (such as `callback`, `hess`, etc.), except the `options` dict, which has\n",
      "    its contents also passed as `method` parameters pair by pair.  Also, if\n",
      "    `jac` has been passed as a bool type, `jac` and `fun` are mangled so that\n",
      "    `fun` returns just the function values and `jac` is converted to a function\n",
      "    returning the Jacobian.  The method shall return an ``OptimizeResult``\n",
      "    object.\n",
      "    \n",
      "    The provided `method` callable must be able to accept (and possibly ignore)\n",
      "    arbitrary parameters; the set of parameters accepted by `minimize` may\n",
      "    expand in future versions and then these parameters will be passed to\n",
      "    the method.  You can find an example in the scipy.optimize tutorial.\n",
      "    \n",
      "    .. versionadded:: 0.11.0\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
      "        Minimization. The Computer Journal 7: 308-13.\n",
      "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
      "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
      "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
      "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
      "        191-208.\n",
      "    .. [3] Powell, M J D. 1964. An efficient method for finding the minimum of\n",
      "       a function of several variables without calculating derivatives. The\n",
      "       Computer Journal 7: 155-162.\n",
      "    .. [4] Press W, S A Teukolsky, W T Vetterling and B P Flannery.\n",
      "       Numerical Recipes (any edition), Cambridge University Press.\n",
      "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
      "       Springer New York.\n",
      "    .. [6] Byrd, R H and P Lu and J. Nocedal. 1995. A Limited Memory\n",
      "       Algorithm for Bound Constrained Optimization. SIAM Journal on\n",
      "       Scientific and Statistical Computing 16 (5): 1190-1208.\n",
      "    .. [7] Zhu, C and R H Byrd and J Nocedal. 1997. L-BFGS-B: Algorithm\n",
      "       778: L-BFGS-B, FORTRAN routines for large scale bound constrained\n",
      "       optimization. ACM Transactions on Mathematical Software 23 (4):\n",
      "       550-560.\n",
      "    .. [8] Nash, S G. Newton-Type Minimization Via the Lanczos Method.\n",
      "       1984. SIAM Journal of Numerical Analysis 21: 770-778.\n",
      "    .. [9] Powell, M J D. A direct search optimization method that models\n",
      "       the objective and constraint functions by linear interpolation.\n",
      "       1994. Advances in Optimization and Numerical Analysis, eds. S. Gomez\n",
      "       and J-P Hennart, Kluwer Academic (Dordrecht), 51-67.\n",
      "    .. [10] Powell M J D. Direct search algorithms for optimization\n",
      "       calculations. 1998. Acta Numerica 7: 287-336.\n",
      "    .. [11] Powell M J D. A view of algorithms for optimization without\n",
      "       derivatives. 2007.Cambridge University Technical Report DAMTP\n",
      "       2007/NA03\n",
      "    .. [12] Kraft, D. A software package for sequential quadratic\n",
      "       programming. 1988. Tech. Rep. DFVLR-FB 88-28, DLR German Aerospace\n",
      "       Center -- Institute for Flight Mechanics, Koln, Germany.\n",
      "    .. [13] Conn, A. R., Gould, N. I., and Toint, P. L.\n",
      "       Trust region methods. 2000. Siam. pp. 169-200.\n",
      "    .. [14] F. Lenders, C. Kirches, A. Potschka: \"trlib: A vector-free\n",
      "       implementation of the GLTR method for iterative solution of\n",
      "       the trust region problem\", https://arxiv.org/abs/1611.04718\n",
      "    .. [15] N. Gould, S. Lucidi, M. Roma, P. Toint: \"Solving the\n",
      "       Trust-Region Subproblem using the Lanczos Method\",\n",
      "       SIAM J. Optim., 9(2), 504--525, (1999).\n",
      "    .. [16] Byrd, Richard H., Mary E. Hribar, and Jorge Nocedal. 1999.\n",
      "        An interior point algorithm for large-scale nonlinear  programming.\n",
      "        SIAM Journal on Optimization 9.4: 877-900.\n",
      "    .. [17] Lalee, Marucha, Jorge Nocedal, and Todd Plantega. 1998. On the\n",
      "        implementation of an algorithm for large-scale equality constrained\n",
      "        optimization. SIAM Journal on Optimization 8.3: 682-706.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let us consider the problem of minimizing the Rosenbrock function. This\n",
      "    function (and its respective derivatives) is implemented in `rosen`\n",
      "    (resp. `rosen_der`, `rosen_hess`) in the `scipy.optimize`.\n",
      "    \n",
      "    >>> from scipy.optimize import minimize, rosen, rosen_der\n",
      "    \n",
      "    A simple application of the *Nelder-Mead* method is:\n",
      "    \n",
      "    >>> x0 = [1.3, 0.7, 0.8, 1.9, 1.2]\n",
      "    >>> res = minimize(rosen, x0, method='Nelder-Mead', tol=1e-6)\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    Now using the *BFGS* algorithm, using the first derivative and a few\n",
      "    options:\n",
      "    \n",
      "    >>> res = minimize(rosen, x0, method='BFGS', jac=rosen_der,\n",
      "    ...                options={'gtol': 1e-6, 'disp': True})\n",
      "    Optimization terminated successfully.\n",
      "             Current function value: 0.000000\n",
      "             Iterations: 26\n",
      "             Function evaluations: 31\n",
      "             Gradient evaluations: 31\n",
      "    >>> res.x\n",
      "    array([ 1.,  1.,  1.,  1.,  1.])\n",
      "    >>> print(res.message)\n",
      "    Optimization terminated successfully.\n",
      "    >>> res.hess_inv\n",
      "    array([[ 0.00749589,  0.01255155,  0.02396251,  0.04750988,  0.09495377],  # may vary\n",
      "           [ 0.01255155,  0.02510441,  0.04794055,  0.09502834,  0.18996269],\n",
      "           [ 0.02396251,  0.04794055,  0.09631614,  0.19092151,  0.38165151],\n",
      "           [ 0.04750988,  0.09502834,  0.19092151,  0.38341252,  0.7664427 ],\n",
      "           [ 0.09495377,  0.18996269,  0.38165151,  0.7664427,   1.53713523]])\n",
      "    \n",
      "    \n",
      "    Next, consider a minimization problem with several constraints (namely\n",
      "    Example 16.4 from [5]_). The objective function is:\n",
      "    \n",
      "    >>> fun = lambda x: (x[0] - 1)**2 + (x[1] - 2.5)**2\n",
      "    \n",
      "    There are three constraints defined as:\n",
      "    \n",
      "    >>> cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] - 2 * x[1] + 6},\n",
      "    ...         {'type': 'ineq', 'fun': lambda x: -x[0] + 2 * x[1] + 2})\n",
      "    \n",
      "    And variables must be positive, hence the following bounds:\n",
      "    \n",
      "    >>> bnds = ((0, None), (0, None))\n",
      "    \n",
      "    The optimization problem is solved using the SLSQP method as:\n",
      "    \n",
      "    >>> res = minimize(fun, (2, 0), method='SLSQP', bounds=bnds,\n",
      "    ...                constraints=cons)\n",
      "    \n",
      "    It should converge to the theoretical solution (1.4 ,1.7).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(minimize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта функция имеет стандартный интерфейс: нужно передать callable объект, который вычисляет значение и градиент целевой функции, а также точку старта оптимизации - начальное приближение (одномерный numpy-массив). Поэтому нам понадобятся функции для сбора и задания всех весов нашей нейросети (именно для них мы всегда записывали параметры слоя в список layer.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(network):\n",
    "    weights = []\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            weights += param.ravel().tolist()\n",
    "    return np.array(weights)\n",
    "\n",
    "def set_weights(weights, network):\n",
    "    i = 0\n",
    "    for layer in network:\n",
    "        for param in layer.params:\n",
    "            l = param.size\n",
    "            param[:] = weights[i:i+l].\\\n",
    "                             reshape(param.shape)\n",
    "            i += l\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вам нужно реализовать ту самую функцию, которую мы будем передавать в minimize. Эта функция должна брать на вход текущую точку (вектор всех параметров), а также список дополнительных параметров (мы будем передавать через них нашу сеть и обучающие данные) и возвращать значение критерия качества (кросс-энтропия) и его градиент по параметрам модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss_grad(weights, args):\n",
    "    \"\"\"\n",
    "    takes current weights and computes cross-entropy and gradients\n",
    "    weights shape: [num_parameters]\n",
    "    output 1: loss (scalar)\n",
    "    output 2: gradint w.r.t. weights, shape: [num_parameters]\n",
    "    \n",
    "    hint: firstly perform forward pass through the whole network\n",
    "    then compute loss and its gradients\n",
    "    then perform backward pass, transmitting first baskward output\n",
    "    to the previos layer and saving second baskward output in a list\n",
    "    finally flatten all the gradients in this list\n",
    "    (in the order from the first to the last layer)\n",
    "    \n",
    "    Do not forget to set weights of the network!\n",
    "    \"\"\"\n",
    "    network, X, y = args\n",
    "    set_weights(weights, network)\n",
    "    curr = X\n",
    "    for layer in network:\n",
    "        curr = layer.forward(curr)\n",
    "    loss = crossentropy(curr, y)\n",
    "    grad_list = []\n",
    "    d_loss_d_curr = []\n",
    "    d_loss_d_curr.append(grad_crossentropy(curr, y))\n",
    "    for layer in reversed(network):\n",
    "        d_loss_d_curr = layer.backward(d_loss_d_curr[0])\n",
    "        if type(d_loss_d_curr[1]) is np.ndarray:\n",
    "            grad_list = d_loss_d_curr[1].tolist() + grad_list\n",
    "        else:\n",
    "            grad_list = d_loss_d_curr[1] + grad_list\n",
    "    return loss, np.array(grad_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы готовы обучать нашу нейросеть. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = get_weights(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3466,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "               args=[network, X_train, y_train_ohe], # args passed to fun\n",
    "               method=\"L-BFGS-B\", # optimization method\n",
    "               jac=True) # says that gradient are computed in fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['fun', 'jac', 'nfev', 'nit', 'status', 'message', 'x', 'success', 'hess_inv'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"nit\"] # number of iterations (should be >> 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"success\"] # should be True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01249765,  0.011819  ,  0.01508702, ..., -0.13799165,\n",
       "       -0.55023413,  0.0583763 ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"x\"] # leraned weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите качество на обучении (X_train, y_train) и на контроле (X_test, y_test). Не забудьте установить веса!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 1.0\n",
      "test accuracy: 0.9511111111111111\n"
     ]
    }
   ],
   "source": [
    "set_weights(res[\"x\"], network)\n",
    "y_pred = predict(network, X_train)\n",
    "print('train accuracy:', np.sum(y_pred == y_train) / np.size(y_train))\n",
    "y_pred = predict(network, X_test)\n",
    "print('test accuracy:', np.sum(y_pred == y_test) / np.size(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У minimize есть также аргумент callback - в нее можно передать функцию, которая будет вызываться после каждой итерации оптимизации. Такую функцию удобно оформить в виде метода класса, который будет сохранять качество на обучении контроле после каждой итерации. Реализуйте этот метод в классе Callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback:\n",
    "    def __init__(self, network, X_train, y_train, X_test, y_test, print=False):\n",
    "        self.network = network\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.print = print\n",
    "        self.train_acc = []\n",
    "        self.test_acc = []\n",
    "        \n",
    "    def call(self, weights):\n",
    "        \"\"\"\n",
    "        computes quality on train and test set with given weights\n",
    "        and saves to self.train_acc and self.test_acc\n",
    "        if self.print is True, also prints these 2 values\n",
    "        \"\"\"\n",
    "        set_weights(weights, self.network)\n",
    "        y_pred = predict(self.network, self.X_train)\n",
    "        curr_train_acc = np.sum(y_pred == self.y_train) / np.size(self.y_train)\n",
    "        self.train_acc.append(curr_train_acc)\n",
    "        y_pred = predict(self.network, self.X_test)\n",
    "        curr_test_acc = np.sum(y_pred == self.y_test) / np.size(self.y_test)\n",
    "        self.test_acc.append(curr_test_acc)\n",
    "        if(self.print):\n",
    "            print('train accuracy:', curr_train_acc)\n",
    "            print('test accuracy:', curr_test_acc)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.11804008908685969\n",
      "test accuracy: 0.09111111111111111\n",
      "train accuracy: 0.23608017817371937\n",
      "test accuracy: 0.2222222222222222\n",
      "train accuracy: 0.21752041573867856\n",
      "test accuracy: 0.21777777777777776\n",
      "train accuracy: 0.25389755011135856\n",
      "test accuracy: 0.23333333333333334\n",
      "train accuracy: 0.28210838901262064\n",
      "test accuracy: 0.27111111111111114\n",
      "train accuracy: 0.29398663697104677\n",
      "test accuracy: 0.2866666666666667\n",
      "train accuracy: 0.42316258351893093\n",
      "test accuracy: 0.37555555555555553\n",
      "train accuracy: 0.4053452115812918\n",
      "test accuracy: 0.37333333333333335\n",
      "train accuracy: 0.3904974016332591\n",
      "test accuracy: 0.4066666666666667\n",
      "train accuracy: 0.5590200445434298\n",
      "test accuracy: 0.4911111111111111\n",
      "train accuracy: 0.6577579806978471\n",
      "test accuracy: 0.6266666666666667\n",
      "train accuracy: 0.7089829250185598\n",
      "test accuracy: 0.6955555555555556\n",
      "train accuracy: 0.7542687453600594\n",
      "test accuracy: 0.7133333333333334\n",
      "train accuracy: 0.7461024498886414\n",
      "test accuracy: 0.7133333333333334\n",
      "train accuracy: 0.7683741648106904\n",
      "test accuracy: 0.7488888888888889\n",
      "train accuracy: 0.7891610987379362\n",
      "test accuracy: 0.7466666666666667\n",
      "train accuracy: 0.799554565701559\n",
      "test accuracy: 0.76\n",
      "train accuracy: 0.8054936896807721\n",
      "test accuracy: 0.7622222222222222\n",
      "train accuracy: 0.8106904231625836\n",
      "test accuracy: 0.7666666666666667\n",
      "train accuracy: 0.8195991091314031\n",
      "test accuracy: 0.7888888888888889\n",
      "train accuracy: 0.8351893095768375\n",
      "test accuracy: 0.7977777777777778\n",
      "train accuracy: 0.8448403860430587\n",
      "test accuracy: 0.8133333333333334\n",
      "train accuracy: 0.8574610244988864\n",
      "test accuracy: 0.8133333333333334\n",
      "train accuracy: 0.8626577579806979\n",
      "test accuracy: 0.8311111111111111\n",
      "train accuracy: 0.8804751299183371\n",
      "test accuracy: 0.8444444444444444\n",
      "train accuracy: 0.8916109873793615\n",
      "test accuracy: 0.8688888888888889\n",
      "train accuracy: 0.9057164068299925\n",
      "test accuracy: 0.8822222222222222\n",
      "train accuracy: 0.9190794357832219\n",
      "test accuracy: 0.8955555555555555\n",
      "train accuracy: 0.9287305122494433\n",
      "test accuracy: 0.9044444444444445\n",
      "train accuracy: 0.933184855233853\n",
      "test accuracy: 0.9133333333333333\n",
      "train accuracy: 0.9376391982182628\n",
      "test accuracy: 0.9133333333333333\n",
      "train accuracy: 0.9376391982182628\n",
      "test accuracy: 0.9155555555555556\n",
      "train accuracy: 0.9465478841870824\n",
      "test accuracy: 0.9266666666666666\n",
      "train accuracy: 0.9539717891610987\n",
      "test accuracy: 0.9333333333333333\n",
      "train accuracy: 0.9502598366740905\n",
      "test accuracy: 0.9377777777777778\n",
      "train accuracy: 0.9591685226429102\n",
      "test accuracy: 0.9377777777777778\n",
      "train accuracy: 0.96362286562732\n",
      "test accuracy: 0.94\n",
      "train accuracy: 0.9688195991091314\n",
      "test accuracy: 0.94\n",
      "train accuracy: 0.9732739420935412\n",
      "test accuracy: 0.9422222222222222\n",
      "train accuracy: 0.9755011135857461\n",
      "test accuracy: 0.9422222222222222\n",
      "train accuracy: 0.9784706755753526\n",
      "test accuracy: 0.9444444444444444\n",
      "train accuracy: 0.9806978470675576\n",
      "test accuracy: 0.9488888888888889\n",
      "train accuracy: 0.9821826280623608\n",
      "test accuracy: 0.9511111111111111\n",
      "train accuracy: 0.9851521900519673\n",
      "test accuracy: 0.9511111111111111\n",
      "train accuracy: 0.9829250185597624\n",
      "test accuracy: 0.9555555555555556\n",
      "train accuracy: 0.9866369710467706\n",
      "test accuracy: 0.9533333333333334\n",
      "train accuracy: 0.9896065330363771\n",
      "test accuracy: 0.9511111111111111\n",
      "train accuracy: 0.9903489235337788\n",
      "test accuracy: 0.9533333333333334\n",
      "train accuracy: 0.9903489235337788\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 0.9933184855233853\n",
      "test accuracy: 0.9622222222222222\n",
      "train accuracy: 0.9933184855233853\n",
      "test accuracy: 0.9622222222222222\n",
      "train accuracy: 0.9955456570155902\n",
      "test accuracy: 0.9622222222222222\n",
      "train accuracy: 0.9955456570155902\n",
      "test accuracy: 0.9622222222222222\n",
      "train accuracy: 0.9962880475129918\n",
      "test accuracy: 0.9644444444444444\n",
      "train accuracy: 0.9977728285077951\n",
      "test accuracy: 0.9666666666666667\n",
      "train accuracy: 0.9977728285077951\n",
      "test accuracy: 0.9666666666666667\n",
      "train accuracy: 0.9977728285077951\n",
      "test accuracy: 0.9688888888888889\n",
      "train accuracy: 0.9992576095025983\n",
      "test accuracy: 0.9666666666666667\n",
      "train accuracy: 0.9992576095025983\n",
      "test accuracy: 0.9711111111111111\n",
      "train accuracy: 0.9992576095025983\n",
      "test accuracy: 0.9711111111111111\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9688888888888889\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9688888888888889\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9688888888888889\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9666666666666667\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9688888888888889\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9666666666666667\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9666666666666667\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9644444444444444\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9666666666666667\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9644444444444444\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9644444444444444\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9622222222222222\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.96\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9622222222222222\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9622222222222222\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.96\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.96\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9644444444444444\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9622222222222222\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9622222222222222\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.96\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9555555555555556\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9555555555555556\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9555555555555556\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9555555555555556\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9577777777777777\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9555555555555556\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9511111111111111\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9511111111111111\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9511111111111111\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9511111111111111\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9511111111111111\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9511111111111111\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9511111111111111\n",
      "train accuracy: 1.0\n",
      "test accuracy: 0.9511111111111111\n"
     ]
    }
   ],
   "source": [
    "cb = Callback(network, X_train, y_train, X_test, y_test, print=True)\n",
    "res = minimize(compute_loss_grad, weights,  \n",
    "               args=[network, X_train, y_train_ohe], \n",
    "               method=\"L-BFGS-B\",\n",
    "               jac=True,\n",
    "               callback=cb.call)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразите на графике кривую качества на обучени ии контроле по итерациям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f1a7c240048>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cb.train_acc, label=\"train acc\")\n",
    "plt.plot(cb.test_acc, label=\"test acc\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты с числом слоев (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ясно, что из-за случайного начального приближения с каждым запуском обучения мы будем получать различное качество. Попробуем обучать нашу нейросеть с разным числом слоев несколько раз.\n",
    "\n",
    "Заполните матрицы accs_train и accs_test. В позиции [i, j] должна стоять величина точности сети с $i+1$ полносвязными слоями при $j$-м запуске (все запуски идентичны)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_train = np.zeros((5, 5))\n",
    "accs_test = np.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daria/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: RuntimeWarning: overflow encountered in exp\n",
      "/home/daria/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:26: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "hidden_layers_size = 32\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if(i == 0):\n",
    "            network = []\n",
    "            network.append(Dense(X_train.shape[1], 10))\n",
    "            network.append(Softmax())\n",
    "        elif(i == 1):\n",
    "            network = []\n",
    "            network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, 10))\n",
    "            network.append(Softmax())\n",
    "        elif(i == 2):\n",
    "            network = []\n",
    "            network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, 10))\n",
    "            network.append(Softmax())\n",
    "        elif(i == 3):\n",
    "            network = []\n",
    "            network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, 10))\n",
    "            network.append(Softmax())\n",
    "        elif(i == 4):\n",
    "            network = []\n",
    "            network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, 10))\n",
    "            network.append(Softmax())\n",
    "        weights = get_weights(network)\n",
    "        res = minimize(compute_loss_grad, weights,  # fun and start point\n",
    "                   args=[network, X_train, y_train_ohe], # args passed to fun\n",
    "                   method=\"L-BFGS-B\", # optimization method\n",
    "                   jac=True) # says that gradient are computed in fun\n",
    "        set_weights(res[\"x\"], network)\n",
    "        y_pred = predict(network, X_train)\n",
    "        accs_train[i, j] = np.sum(y_pred == y_train) / np.size(y_train)\n",
    "        y_pred = predict(network, X_test)\n",
    "        accs_test[i, j] = np.sum(y_pred == y_test) / np.size(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94222222, 0.94222222, 0.94888889, 0.94444444, 0.94666667],\n",
       "       [0.95111111, 0.95333333, 0.96444444, 0.95777778, 0.95111111],\n",
       "       [0.96444444, 0.95555556, 0.94888889, 0.94444444, 0.95333333],\n",
       "       [0.20444444, 0.95111111, 0.10222222, 0.18888889, 0.94888889],\n",
       "       [0.1       , 0.95777778, 0.94888889, 0.09777778, 0.18666667]])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим боксплоты полученного качества (горизонтальная линия в каждом столбце - среднее, прямоугольник показывает разброс)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Test quality in 5 runs')"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(accs_test.T, showfliers=False)\n",
    "plt.xlabel(\"Num layers\")\n",
    "plt.ylabel(\"Test accuracy\")\n",
    "plt.title(\"Test quality in 5 runs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Train quality in 5 runs')"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(accs_train.T, showfliers=False)\n",
    "plt.xlabel(\"Num layers\")\n",
    "plt.ylabel(\"Train accuracy\")\n",
    "plt.title(\"Train quality in 5 runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопросы (кратко в этой же ячейке):\n",
    "* Как изменяются качество на обучении и контроле и устойчивость процесса обучения при увеличении числа слоев? \n",
    "\n",
    "Устойчивость на обучении и контроле падает, лучшая точность и наиболее стабильные результаты достигаются на небольшом числе слоев.\n",
    "* Можно ли сказать, что логистическая регрессия (линейная модель) дает качество хуже, чем нелинейная модель?\n",
    "\n",
    "Возможно, использование нелинейной модели приводит к появлению у оптимизируемой функции большого числа локальных минимумов, поэтому все большее количество точек начального приближения не будет приводить к сходимости к глобальной точке оптимума. При правильном выборе начального приближения можно добиться хорошего качества и с использованием нелинейной модели, но вероятность такого выбора уменьшается с ростом числа слоев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* Несколько фрагментов кода в задании написаны на основе материалов [курса по глубинному обучению на ФКН НИУ ВШЭ](https://www.hse.ru/ba/ami/courses/205504078.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусная часть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Реализация метода оптимизации (1 балл)\n",
    "\n",
    "Реализуйте сами метод оптимизации (аналог функции minimize) для рассмотренной выше архитектуры. В качестве метода оптимизации используйте SGD + momentum. Продемонстрируйте правильную работу метода оптимизации, сравните его работы с LBFGS-B. Сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd_momentum(compute_loss_grad, weights, network, X_train, y_train, batch_size=10, \n",
    "                 alpha=0.8, beta=0.01, max_iter=2000, loss_step=50, callback = None):\n",
    "    indexes = np.arange(np.size(y_train, 0))\n",
    "    np.random.shuffle(indexes)\n",
    "    curr_weights = weights\n",
    "    curr_ind = 0\n",
    "    num_iterations = 0\n",
    "    last_loss = 0\n",
    "    curr_loss = 0\n",
    "    momentum = 0\n",
    "    loss_counter = 0\n",
    "    while(num_iterations == 0 or\n",
    "          (num_iterations < max_iter)):\n",
    "        last_loss = curr_loss\n",
    "        if(curr_ind >= np.size(y_train, 0)):\n",
    "            indexes = np.arange(np.size(y_train, 0))\n",
    "            np.random.shuffle(indexes)\n",
    "            curr_ind = 0\n",
    "        curr_loss, curr_grad = compute_loss_grad(curr_weights, \n",
    "                    [\n",
    "                        network, \n",
    "                        X_train[indexes[curr_ind:curr_ind + batch_size],:], \n",
    "                        y_train[indexes[curr_ind:curr_ind + batch_size],:]\n",
    "                    ])\n",
    "        curr_ind += batch_size\n",
    "        momentum = (1 - alpha) * momentum + alpha * curr_grad\n",
    "        curr_weights = curr_weights - beta * momentum\n",
    "        num_iterations += 1\n",
    "        if(callback):\n",
    "            callback.call(curr_weights)\n",
    "        if(curr_loss < last_loss):\n",
    "            loss_counter += 1\n",
    "            if(loss_counter == loss_step):\n",
    "                beta /= 2\n",
    "                loss_counter = 0\n",
    "    return curr_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "accs_train = np.zeros((5, 5))\n",
    "accs_test = np.zeros((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layers_size = 32\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(5):\n",
    "        if(i == 0):\n",
    "            network = []\n",
    "            network.append(Dense(X_train.shape[1], 10))\n",
    "            network.append(Softmax())\n",
    "        elif(i == 1):\n",
    "            network = []\n",
    "            network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, 10))\n",
    "            network.append(Softmax())\n",
    "        elif(i == 2):\n",
    "            network = []\n",
    "            network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, 10))\n",
    "            network.append(Softmax())\n",
    "        elif(i == 3):\n",
    "            network = []\n",
    "            network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, 10))\n",
    "            network.append(Softmax())\n",
    "        elif(i == 4):\n",
    "            network = []\n",
    "            network.append(Dense(X_train.shape[1], hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, hidden_layers_size))\n",
    "            network.append(ReLU())\n",
    "            network.append(Dense(hidden_layers_size, 10))\n",
    "            network.append(Softmax())\n",
    "        weights = get_weights(network)\n",
    "        if(i < 2):\n",
    "            alpha = 0.1\n",
    "            max_iter = 3000\n",
    "            loss_step = 50\n",
    "        elif(i == 2):\n",
    "            alpha = 0.8\n",
    "            max_iter = 3000\n",
    "            loss_step = 200\n",
    "        elif(i == 3):\n",
    "            alpha = 0.8\n",
    "            max_iter = 5000\n",
    "            loss_step = 400\n",
    "        elif(i == 4):\n",
    "            alpha = 0.8\n",
    "            max_iter = 5000\n",
    "            loss_step = 350\n",
    "        weights = sgd_momentum(compute_loss_grad, weights, network, X_train, y_train_ohe,\n",
    "                               alpha=alpha, beta=0.01, max_iter=max_iter, loss_step=loss_step)\n",
    "        set_weights(weights, network)\n",
    "        y_pred = predict(network, X_train)\n",
    "        accs_train[i, j] = np.sum(y_pred == y_train) / np.size(y_train)\n",
    "        y_pred = predict(network, X_test)\n",
    "        accs_test[i, j] = np.sum(y_pred == y_test) / np.size(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.96666667, 0.95333333, 0.96444444, 0.95777778, 0.96444444],\n",
       "       [0.96666667, 0.96222222, 0.95777778, 0.96666667, 0.95555556],\n",
       "       [0.83111111, 0.96666667, 0.94      , 0.86      , 0.94666667],\n",
       "       [0.96444444, 0.96      , 0.96888889, 0.96444444, 0.95333333],\n",
       "       [0.94888889, 0.96222222, 0.94666667, 0.96222222, 0.95555556]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98292502, 0.98144024, 0.97772829, 0.97847068, 0.97921307],\n",
       "       [0.98218263, 0.98589458, 0.98515219, 0.98663697, 0.98589458],\n",
       "       [0.88938382, 0.9925761 , 0.98737936, 0.90274684, 0.97847068],\n",
       "       [0.99925761, 0.99703044, 0.99777283, 0.99331849, 0.99777283],\n",
       "       [0.9918337 , 1.        , 0.99703044, 0.99628805, 0.99851522]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accs_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как известно, стохастический градиентный спуск не может сойтись полностью, тк даже в точке оптимума градиент не будет равен нулю, поэтому точность на обучающей выборке с очень маленькой вероятностью будет равна 1.0. Но он намного стабильнее метода L-BFGS-B, меньше зависит от начального приближения и при правильно подобранных параметрах alpha, beta, max_iter, loss_step позволяет добиться высокой точности на тестовой выборке, превышающей точность предыдущего метода, даже при большом количестве слоев нелинейности. Это может быть связано с тем, что использование инерции позволяет \"перескочить\" локальные минимумы функции потерь и при любом начальном приближении стремиться к глобальному оптимуму."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout (1 балл) \n",
    "\n",
    "Реализуйте слой Dropout. Сравните обучение сети из большого числа слоёв при использовании Dropout и без его использования (предварительно подберите адекватный параметр p). Сделайте выводы. Используя метод оптимизации из первого бонусного пункта. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        self.params = []\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.bern_vec = bernoulli.rvs(p=1 - self.p, size=np.shape(input)).reshape(np.shape(input))\n",
    "        return input * self.bern_vec\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        return 1 / (1 - self.p) * self.bern_vec * grad_output, []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не удалось добиться улучшения качества при использовании слоя Dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BatchNormalization (1 балл)\n",
    "\n",
    "Реализуйте слой BatchNormalization. Сравните обучение сети из большого числа слоёв при использовании BatchNormalization и без его использования.  Сделайте выводы. Используя метод оптимизации из первого бонусного пункта. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, input_size):\n",
    "        self.gamma = np.random.normal(loc=1, size=(input_size))\n",
    "        self.beta = np.random.normal(loc=0, size=(input_size))\n",
    "        self.params = [self.beta, self.gamma]\n",
    "        \n",
    "    def forward(self, input):\n",
    "        print('input', input)\n",
    "        self.mean = (1. / np.size(input, 0)) * np.sum(input, axis=0)\n",
    "        var = (1. / np.size(input, 0)) * np.sum((input - self.mean[np.newaxis, :]) ** 2, axis=0)\n",
    "        eps = 1e-8\n",
    "        self.div = 1. / np.sqrt(var + eps)\n",
    "        self.x_hat = (input - self.mean[np.newaxis, :]) * self.div[np.newaxis, :]\n",
    "        print('!!', self.x_hat)\n",
    "        return self.gamma[np.newaxis, :] * self.x_hat + self.beta[np.newaxis, :]\n",
    "    \n",
    "    def backward(self, grad_output):\n",
    "        d_loss_d_beta = np.sum(grad_output, axis=0)\n",
    "        d_loss_d_gamma = np.sum(grad_output * self.x_hat, axis=0)\n",
    "        d_loss_d_xhat = grad_output * self.gamma[np.newaxis, :]\n",
    "        N = np.size(grad_output, 0)\n",
    "        d_loss_d_input = (1. / N) * self.div * (N * d_loss_d_xhat - np.sum(d_loss_d_xhat, axis=0)[np.newaxis,:] - \n",
    "                                self.x_hat * np.sum(d_loss_d_xhat * self.x_hat, axis=0)[np.newaxis, :])\n",
    "        #d_loss_d_input = (1. / N) * self.div * self.gamma *(- d_loss_d_gamma * self.x_hat + \n",
    "        #                                N * grad_output - d_loss_d_beta)\n",
    "        return d_loss_d_input, np.r_[d_loss_d_beta.ravel(), d_loss_d_gamma.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [[-1.         -0.98319328 -0.96638655 -0.94957983 -0.93277311 -0.91596639\n",
      "  -0.89915966 -0.88235294 -0.86554622 -0.8487395  -0.83193277 -0.81512605]\n",
      " [-0.79831933 -0.78151261 -0.76470588 -0.74789916 -0.73109244 -0.71428571\n",
      "  -0.69747899 -0.68067227 -0.66386555 -0.64705882 -0.6302521  -0.61344538]\n",
      " [-0.59663866 -0.57983193 -0.56302521 -0.54621849 -0.52941176 -0.51260504\n",
      "  -0.49579832 -0.4789916  -0.46218487 -0.44537815 -0.42857143 -0.41176471]\n",
      " [-0.39495798 -0.37815126 -0.36134454 -0.34453782 -0.32773109 -0.31092437\n",
      "  -0.29411765 -0.27731092 -0.2605042  -0.24369748 -0.22689076 -0.21008403]\n",
      " [-0.19327731 -0.17647059 -0.15966387 -0.14285714 -0.12605042 -0.1092437\n",
      "  -0.09243697 -0.07563025 -0.05882353 -0.04201681 -0.02521008 -0.00840336]\n",
      " [ 0.00840336  0.02521008  0.04201681  0.05882353  0.07563025  0.09243697\n",
      "   0.1092437   0.12605042  0.14285714  0.15966387  0.17647059  0.19327731]\n",
      " [ 0.21008403  0.22689076  0.24369748  0.2605042   0.27731092  0.29411765\n",
      "   0.31092437  0.32773109  0.34453782  0.36134454  0.37815126  0.39495798]\n",
      " [ 0.41176471  0.42857143  0.44537815  0.46218487  0.4789916   0.49579832\n",
      "   0.51260504  0.52941176  0.54621849  0.56302521  0.57983193  0.59663866]\n",
      " [ 0.61344538  0.6302521   0.64705882  0.66386555  0.68067227  0.69747899\n",
      "   0.71428571  0.73109244  0.74789916  0.76470588  0.78151261  0.79831933]\n",
      " [ 0.81512605  0.83193277  0.8487395   0.86554622  0.88235294  0.89915966\n",
      "   0.91596639  0.93277311  0.94957983  0.96638655  0.98319328  1.        ]]\n",
      "!! [[-1.56669888 -1.56669888 -1.56669888 -1.56669888 -1.56669888 -1.56669888\n",
      "  -1.56669888 -1.56669888 -1.56669888 -1.56669888 -1.56669888 -1.56669888]\n",
      " [-1.21854357 -1.21854357 -1.21854357 -1.21854357 -1.21854357 -1.21854357\n",
      "  -1.21854357 -1.21854357 -1.21854357 -1.21854357 -1.21854357 -1.21854357]\n",
      " [-0.87038827 -0.87038827 -0.87038827 -0.87038827 -0.87038827 -0.87038827\n",
      "  -0.87038827 -0.87038827 -0.87038827 -0.87038827 -0.87038827 -0.87038827]\n",
      " [-0.52223296 -0.52223296 -0.52223296 -0.52223296 -0.52223296 -0.52223296\n",
      "  -0.52223296 -0.52223296 -0.52223296 -0.52223296 -0.52223296 -0.52223296]\n",
      " [-0.17407765 -0.17407765 -0.17407765 -0.17407765 -0.17407765 -0.17407765\n",
      "  -0.17407765 -0.17407765 -0.17407765 -0.17407765 -0.17407765 -0.17407765]\n",
      " [ 0.17407765  0.17407765  0.17407765  0.17407765  0.17407765  0.17407765\n",
      "   0.17407765  0.17407765  0.17407765  0.17407765  0.17407765  0.17407765]\n",
      " [ 0.52223296  0.52223296  0.52223296  0.52223296  0.52223296  0.52223296\n",
      "   0.52223296  0.52223296  0.52223296  0.52223296  0.52223296  0.52223296]\n",
      " [ 0.87038827  0.87038827  0.87038827  0.87038827  0.87038827  0.87038827\n",
      "   0.87038827  0.87038827  0.87038827  0.87038827  0.87038827  0.87038827]\n",
      " [ 1.21854357  1.21854357  1.21854357  1.21854357  1.21854357  1.21854357\n",
      "   1.21854357  1.21854357  1.21854357  1.21854357  1.21854357  1.21854357]\n",
      " [ 1.56669888  1.56669888  1.56669888  1.56669888  1.56669888  1.56669888\n",
      "   1.56669888  1.56669888  1.56669888  1.56669888  1.56669888  1.56669888]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-68b3caaf242a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m#print(numeric_grads.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x = np.linspace(-1, 1, 10*12).reshape([10, 12])\n",
    "bn = BatchNormalization(12)\n",
    "\n",
    "def func(x):\n",
    "    gamma = bn.gamma\n",
    "    beta = bn.beta\n",
    "    mean = (1. / np.size(x, 0)) * np.sum(x, axis=0)\n",
    "    div = (1. / np.size(x, 0)) * np.sum((x - mean[np.newaxis, :]) ** 2, axis=0)\n",
    "    x_hat = (x - mean[np.newaxis, :]) / (np.sqrt(div + 1e-8))[np.newaxis, :]\n",
    "    #print(x_hat)\n",
    "    return np.sum(gamma[np.newaxis, :] * x_hat + beta[np.newaxis, :])\n",
    "    \n",
    "d_loss_d_layer = np.ones((10, 12))\n",
    "bn.forward(x)\n",
    "grads = bn.backward(d_loss_d_layer)[0]\n",
    "#print(grads.shape)\n",
    "\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(func, x)\n",
    "#print(numeric_grads.shape)\n",
    "\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание не доделано, неправильно вычислен градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
